
# π¤– ChatOllama μ¤νΈλ¦¬λ° μ±„ν… μ‹μ¤ν…

ChatOllamaλ¥Ό ν™μ©ν• **μ‹¤μ‹κ°„ μ¤νΈλ¦¬λ° AI μ±„ν…** μ›Ή μ• ν”λ¦¬μΌ€μ΄μ…μ…λ‹λ‹¤. 

## β¨ μ£Όμ” κΈ°λ¥

### π”„ **μ‹¤μ‹κ°„ μ¤νΈλ¦¬λ° μ‘λ‹µ**
- ChatOllama λ¨λΈμ„ ν†µν• ν† ν° λ‹¨μ„ μ‹¤μ‹κ°„ μ‘λ‹µ μƒμ„±
- μ‚¬μ©μκ°€ μ§λ¬Έν•λ” μ¦‰μ‹ AIκ°€ μ‹¤μ‹κ°„μΌλ΅ λ‹µλ³€μ„ μ¤νΈλ¦¬λ°

### π’¬ **λ€ν™” νμ¤ν† λ¦¬ κ΄€λ¦¬**
- **μ„λ²„**: μµκ·Ό 4κ° λ©”μ‹μ§€ μμ„ λ©”λ¨λ¦¬μ— μ μ§€ν•μ—¬ μ»¨ν…μ¤νΈ μ—°μ†μ„± λ³΄μ¥
- **ν΄λΌμ΄μ–ΈνΈ**: μ „μ²΄ λ€ν™” λ‚΄μ©μ„ μ‹κ°μ μΌλ΅ ν‘μ‹ λ° λ΅μ»¬ κ΄€λ¦¬
- λ©”λ¨λ¦¬ ν¨μ¨μ μΈ μλ™ νμ¤ν† λ¦¬ μ •λ¦¬

### π― **μ¤λ§νΈ ν”„λ΅¬ν”„νΈ μ‹μ¤ν…**
- μ‘λ‹µ μ™„λ£ ν›„ κ³ μ •λ μ¶”μ² ν”„λ΅¬ν”„νΈ μλ™ μ κ³µ:
  - "λ” κµ¬μ²΄μ μΌλ΅ λ‹µλ³€ν•΄ μ£Όμ„Έμ”."
  - "λ‹¤λ¥Έ μκ²¬λ„ λ“£κ³  μ‹¶μ–΄μ”."
  - "μ§§κ² μ”μ•½ν•΄ μ£Όμ„Έμ”."

### π†• **μƒ μ±„ν… κΈ°λ¥**
- "μƒ μ±„ν…" λ²„νΌμΌλ΅ μ„λ²„μ™€ ν΄λΌμ΄μ–ΈνΈ λ€ν™” νμ¤ν† λ¦¬ λ™μ‹ μ΄κΈ°ν™”
- μ–Έμ λ“ μ§€ μƒλ΅μ΄ μ£Όμ λ΅ λ€ν™” μ‹μ‘ κ°€λ¥

### π¨ **μ‚¬μ©μ μΉν™”μ  UI**
- μ‹¤μ‹κ°„ λ€ν™” μƒνƒ ν‘μ‹ (μ—°κ²°λ¨, μ‘λ‹µ μƒμ„± μ¤‘, μ¤λ¥ λ“±)
- μ΄ λ©”μ‹μ§€ κ°μ μΉ΄μ΄ν„°
- μ‚¬μ©μ/AI λ©”μ‹μ§€ μƒ‰μƒ κµ¬λ¶„ ν‘μ‹
- μ¤ν¬λ΅¤ κ°€λ¥ν• λ€ν™” νμ¤ν† λ¦¬ μμ—­

![screenshot](https://github.com/user-attachments/assets/b5db91e6-c6f3-4aed-aeae-37cf4a5a31e9)

## π—οΈ μ‹μ¤ν… κµ¬μ΅°

```
β”β”€β”€ llm_stream_server.py    # FastAPI λ°±μ—”λ“ μ„λ²„
β”β”€β”€ llm_stream_client.html  # μ›Ή ν΄λΌμ΄μ–ΈνΈ μΈν„°νμ΄μ¤
β””β”€β”€ README.md              # ν”„λ΅μ νΈ λ¬Έμ„
```

### π”§ **κΈ°μ  μ¤νƒ**

**λ°±μ—”λ“ (llm_stream_server.py)**
- **FastAPI**: κ³ μ„±λ¥ μ›Ή ν”„λ μ„μ›ν¬
- **ChatOllama**: LangChain κΈ°λ° Ollama μ±„ν… λ¨λΈ
- **Streaming Response**: μ‹¤μ‹κ°„ ν† ν° μ¤νΈλ¦¬λ°
- **CORS λ―Έλ“¤μ›¨μ–΄**: μ›Ή λΈλΌμ°μ € μ ‘κ·Ό ν—μ©

**ν”„λ΅ νΈμ—”λ“ (llm_stream_client.html)**
- **Vanilla JavaScript**: μμ μλ°”μ¤ν¬λ¦½νΈ (ν”„λ μ„μ›ν¬ μ—†μ)
- **Fetch API**: μ„λ²„ ν†µμ‹ 
- **ReadableStream**: μ‹¤μ‹κ°„ μ¤νΈλ¦¬λ° λ°μ΄ν„° μ²λ¦¬
- **λ΅μ»¬ νμ¤ν† λ¦¬ κ΄€λ¦¬**: ν΄λΌμ΄μ–ΈνΈ μ‚¬μ΄λ“ λ€ν™” κΈ°λ΅

## π“‹ μ‚¬μ „ μ”κµ¬μ‚¬ν•­

### 1. **Ollama μ„¤μΉ λ° μ„¤μ •**
```bash
# Ollama μ„¤μΉ (https://ollama.ai/)
curl -fsSL https://ollama.ai/install.sh | sh

# Llama-3-Korean-Bllossom λ¨λΈ λ‹¤μ΄λ΅λ“
ollama pull hf.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M:Q4_K_M
```

### 2. **Python ν¨ν‚¤μ§€ μ„¤μΉ**
```bash
pip install fastapi uvicorn langchain-community
```

## π€ μ„¤μΉ λ° μ‹¤ν–‰

### 1. **μ €μ¥μ† ν΄λ΅ **
```bash
git clone https://github.com/hwantage/fastapi_llm_streaming.git
cd fastapi_llm_streaming
```

### 2. **μ„λ²„ μ‹¤ν–‰**
```bash
python llm_stream_server.py
```
- μ„λ²„κ°€ `http://localhost:8001`μ—μ„ μ‹¤ν–‰λ©λ‹λ‹¤.

### 3. **ν΄λΌμ΄μ–ΈνΈ μ‹¤ν–‰**
- μ›Ή λΈλΌμ°μ €μ—μ„ `llm_stream_client.html` νμΌμ„ μ§μ ‘ μ—΄μ–΄ ν…μ¤νΈν•©λ‹λ‹¤.


## π“„ λΌμ΄μ„ μ¤

MIT
